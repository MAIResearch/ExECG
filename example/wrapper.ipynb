{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TorchModelWrapper Example\n",
    "\n",
    "This notebook explains how to use ExECG's `TorchModelWrapper` with various examples.\n",
    "\n",
    "## Contents\n",
    "1. Basic Usage\n",
    "2. Using Registered Models (afib_binary, potassium_regression)\n",
    "3. Using Preprocess (input shape transformation)\n",
    "4. Using Postprocess (output shape transformation)\n",
    "5. Special Case Model Examples\n",
    "6. Extracting Gradients and Layer Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from execg.models.wrapper import TorchModelWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Basic Usage\n",
    "\n",
    "### Input/Output Convention\n",
    "- **Input**: `(1, n_leads, seq_length)` e.g., `(1, 12, 2500)`\n",
    "- **Output**: `(1, N)` where N is:\n",
    "  - Regression: N=1\n",
    "  - Binary: N=2 (probabilities)\n",
    "  - Multiclass/Multilabel: N=num_classes (probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple test model\n",
    "class SimpleBinaryModel(nn.Module):\n",
    "    \"\"\"Simple Binary Classification model (standard input/output)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(12, 32, kernel_size=7, padding=3)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (1, 12, length)\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=-1)  # (1, 2)\n",
    "\n",
    "\n",
    "# Create and wrap model\n",
    "model = SimpleBinaryModel()\n",
    "wrapper = TorchModelWrapper(model)\n",
    "\n",
    "print(f\"Wrapper: {wrapper}\")\n",
    "print(f\"Device: {wrapper.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction test\n",
    "ecg = torch.randn(1, 12, 2500)  # (1, lead, length)\n",
    "output = wrapper.predict(ecg)\n",
    "\n",
    "print(f\"Input shape: {ecg.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Using Registered Models\n",
    "\n",
    "Examples of using pre-trained models registered in ExECG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from execg.misc import get_model\n",
    "from samples.models.registry import MODEL_REGISTRY\n",
    "\n",
    "print(f\"Available models: {list(MODEL_REGISTRY.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 2.1 AFib Binary Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Load AFib classification model\nafib_model = get_model(\n    name=\"afib_binary\",\n    model_dir=\"../tmp/models/afib_binary/\",\n    registry=MODEL_REGISTRY,\n    download=True,\n)\n\n# Wrap (apply softmax with postprocess)\nafib_wrapper = TorchModelWrapper(afib_model)\n\nprint(f\"Model: {afib_wrapper}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction test\n",
    "ecg_afib = torch.randn(1, 12, 2500)\n",
    "pred = afib_wrapper.predict(ecg_afib)\n",
    "\n",
    "print(f\"Input shape: {ecg_afib.shape}\")\n",
    "print(f\"Output shape: {pred.shape}\")\n",
    "print(f\"Prediction (prob): {pred}\")\n",
    "print(\n",
    "    f\"Predicted class: {pred.argmax().item()} ({'AFib' if pred.argmax().item() == 1 else 'Normal'})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 2.2 Potassium Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Load Potassium regression model\npotassium_model = get_model(\n    name=\"potassium_regression\",\n    model_dir=\"../tmp/models/potassium_regression/\",\n    registry=MODEL_REGISTRY,\n    download=True,\n)\n\npotassium_wrapper = TorchModelWrapper(potassium_model)\n\nprint(f\"Model: {potassium_wrapper}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction test\n",
    "ecg_k = torch.randn(1, 12, 2500)\n",
    "pred_k = potassium_wrapper.predict(ecg_k)\n",
    "\n",
    "print(f\"Input shape: {ecg_k.shape}\")\n",
    "print(f\"Output shape: {pred_k.shape}\")\n",
    "print(f\"Predicted potassium level: {pred_k.item():.2f} mEq/L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 3. Using Preprocess\n",
    "\n",
    "Use `preprocess` when the model expects a different input shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### 3.1 Case: Model expects (1, length, lead) instead of (1, lead, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposedInputModel(nn.Module):\n",
    "    \"\"\"Model that receives input in (1, length, lead) shape\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(12, 64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (1, length, 12)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return F.softmax(self.fc(h.squeeze(0)), dim=-1)\n",
    "\n",
    "\n",
    "# Create model\n",
    "transposed_model = TransposedInputModel()\n",
    "\n",
    "# Use preprocess to transform (1, lead, length) -> (1, length, lead)\n",
    "transposed_wrapper = TorchModelWrapper(\n",
    "    transposed_model,\n",
    "    preprocess=lambda x: x.transpose(1, 2),  # (1, 12, L) -> (1, L, 12)\n",
    ")\n",
    "\n",
    "# Test: use standard input (1, 12, 2500)\n",
    "ecg = torch.randn(1, 12, 2500)\n",
    "output = transposed_wrapper.predict(ecg)\n",
    "\n",
    "print(f\"Standard input shape: {ecg.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 4. Using Postprocess\n",
    "\n",
    "Transform model output to standard format `(1, N)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### 4.1 Case: Model outputs single logit (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLogitModel(nn.Module):\n",
    "    \"\"\"Binary model that outputs a single logit\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(12, 32, kernel_size=7, padding=3)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(32, 1)  # single logit\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)  # (1, 1) logit\n",
    "\n",
    "\n",
    "single_logit_model = SingleLogitModel()\n",
    "\n",
    "\n",
    "# Use postprocess to transform single logit -> (1, 2) binary probs\n",
    "def logit_to_binary_probs(x):\n",
    "    prob_pos = torch.sigmoid(x)\n",
    "    prob_neg = 1 - prob_pos\n",
    "    return torch.cat([prob_neg, prob_pos], dim=-1)  # (1, 2)\n",
    "\n",
    "\n",
    "single_logit_wrapper = TorchModelWrapper(\n",
    "    single_logit_model, postprocess=logit_to_binary_probs\n",
    ")\n",
    "\n",
    "# Test\n",
    "ecg = torch.randn(1, 12, 2500)\n",
    "output = single_logit_wrapper.predict(ecg)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output (binary probs): {output}\")\n",
    "print(f\"Sum of probs: {output.sum().item():.4f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 4.2 Case: prob + auxiliary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilabelModel(nn.Module):\n",
    "    \"\"\"Multilabel classification model\"\"\"\n",
    "\n",
    "    def __init__(self, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(12, 32, kernel_size=7, padding=3)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(32, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return torch.sigmoid(self.fc(x)), {}  # (1, num_labels) logits and etc\n",
    "\n",
    "\n",
    "multilabel_model = MultilabelModel(num_labels=4)\n",
    "\n",
    "# Use postprocess to apply sigmoid (independent probability for each label)\n",
    "multilabel_wrapper = TorchModelWrapper(multilabel_model, postprocess=lambda x: x[0])\n",
    "\n",
    "# Test\n",
    "ecg = torch.randn(1, 12, 2500)\n",
    "output = multilabel_wrapper.predict(ecg)\n",
    "\n",
    "labels = [\"AFib\", \"LBBB\", \"RBBB\", \"PVC\"]\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output (multilabel probs): {output}\")\n",
    "print(\"\\nPredicted labels:\")\n",
    "for i, (label, prob) in enumerate(zip(labels, output[0])):\n",
    "    print(f\"  {label}: {prob.item():.3f} {'*' if prob > 0.5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 5. Extracting Gradients and Layer Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple model\n",
    "class GradTestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(12, 32, kernel_size=7, padding=3)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=7, padding=3)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return F.softmax(self.fc(x), dim=-1)\n",
    "\n",
    "\n",
    "grad_model = GradTestModel()\n",
    "grad_wrapper = TorchModelWrapper(grad_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### 5.1 get_layer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = grad_wrapper.get_layer_names()\n",
    "print(\"Available layers:\")\n",
    "for name in layer_names:\n",
    "    print(f\"  - {name if name else '(root)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### 5.2 get_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = torch.randn(1, 12, 2500)\n",
    "\n",
    "# Gradient for a specific class\n",
    "grads = grad_wrapper.get_gradients(ecg, target_class=1)\n",
    "\n",
    "print(f\"Input shape: {ecg.shape}\")\n",
    "print(f\"Gradient shape: {grads.shape}\")\n",
    "print(f\"Gradient range: [{grads.min():.6f}, {grads.max():.6f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If target_class=None, uses argmax class\n",
    "grads_auto = grad_wrapper.get_gradients(ecg, target_class=None)\n",
    "print(f\"Auto target gradient shape: {grads_auto.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### 5.3 get_layer_gradients() - for Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations and gradients from conv2 layer\n",
    "activations, gradients = grad_wrapper.get_layer_gradients(\n",
    "    ecg, target_class=1, layer_name=\"conv2\"\n",
    ")\n",
    "\n",
    "print(f\"Activations shape: {activations.shape}\")\n",
    "print(f\"Gradients shape: {gradients.shape}\")\n",
    "print(f\"\\nThese can be used for Grad-CAM computation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### 5.4 Using output_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = torch.randn(1, 12, 2500)\n",
    "\n",
    "# Full output\n",
    "full_output = grad_wrapper.predict(ecg)\n",
    "print(f\"Full output: {full_output}\")\n",
    "\n",
    "# Specific index only\n",
    "output_0 = grad_wrapper.predict(ecg, output_idx=0)\n",
    "output_1 = grad_wrapper.predict(ecg, output_idx=1)\n",
    "\n",
    "print(f\"Output[0]: {output_0}\")\n",
    "print(f\"Output[1]: {output_1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}